{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we were discussing recursion we had two ways of doing things. We either used dictionaries for a top-down/memoization approach or a bottom-up/tabulation approach using array indices. Both are valid and both can be quite fast if done correctly. Memoization tends to be easier to read and write while tabulation tends to require less memory. Both should have approximately the same Big O runtime if they are not resource constrained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programs\n",
    "Let's recall our bellman equation\n",
    "\n",
    "## Anatomy of a Dynamic Program\n",
    "In order to make programming dynamic programs easier we should have the following\n",
    "1. Base Case: a case for which the problem is trivial. In fibonacci this was the first two terms.\n",
    "2. State: We want to organize our problem in such a way that we can have unique states. In the case of the egg drop we identified this as the number of floors we have remaining and the number of eggs we have left. States are independent of path, because once you get to a state the optimal behavior for the future does not depend on how you got there. Someone who won a lottery and now has \\$1 million in their bank account should the same to a billionaire who lost their company and now has only \\$1 million remaining.\n",
    "3. Overall Value function: This is the recursive call. The value of a current policy decision is always going to be determined by the immediate value gained from a policy decision plus all future value that can be obtained by acting optimally. This is represented by the Bellman Equation.\n",
    "$$\\underbrace{V^*(s_t)}_{\\text{optimal value}} = \\underset{a}{max}\\left \\{ \\underbrace{R(a,s_t)}_{\\text{Immediate Value Function}} +\n",
    "   \\underbrace{\\sum_{i>t}V^*(s_{i})}_{\\text{Optimal Value of future states}} \\right\\}$$\n",
    "4. Immediate Value function: This is the value that is immediately obtained from a single decision given a single state\n",
    "5. Policy Decision: Policy decisions are the options available to you at a given state. We maximize our objective by choosing the optimal policy decisions.\n",
    "6. Lookup/cache: to prevent repeated calculations you should have some way to store older results and extract them"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Top Down Memoization\n",
    "So we have our equation as listed above. We can use memoization and recursion with a dictionary to create simple code that performs the task beautifully"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "cache = {}\n",
    "def OptimalValue(state):\n",
    "    if state in cache:\n",
    "        return cache[state]\n",
    "    else:\n",
    "        optimal_decision = None\n",
    "        max_value = -1\n",
    "        for decision in possible_decisions:\n",
    "            new_state = transition(state, decision)\n",
    "            value = immediate_value(state,decision) + OptimalValue(new_state)\n",
    "            if value > max_value:\n",
    "                max_value = value\n",
    "                optimal_decision = decision  \n",
    "    cache[state] = (value, decision)\n",
    "    return cache[state]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If everything was done correctly then the end result will be a lookup table that can find you the solution for any state that is included in the range. Usually your solution will be in one of the corners of the problem."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}